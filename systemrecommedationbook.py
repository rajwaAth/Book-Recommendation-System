# -*- coding: utf-8 -*-
"""SystemRecommedationBook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TtCNl9RkmLUZpucSNc13penc_a5tGYBw

# System Recommendation Books

## Background

Di era informasi yang berkembang pesat, dunia literasi menghadapi tantangan baru. Setiap hari, ribuan buku baru diterbitkan di seluruh dunia, mencakup berbagai genre, topik, dan gaya penulisan. Bagi pembaca, keberlimpahan pilihan ini bukannya mempercepat pencarian bacaan, justru seringkali menimbulkan kebingungan: "Buku apa yang sebaiknya saya baca selanjutnya?"

Berangkat dari permasalahan ini, lahirlah ide untuk membangun sebuah Sistem Rekomendasi Buku. Tujuan utamanya sederhana namun krusial: membantu pembaca menemukan buku yang sesuai dengan minat, kebiasaan membaca, dan preferensi pribadi mereka dengan cepat dan tepat.

## Importing Library
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

!pip install opendatasets
import opendatasets as od

od.download('https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset?select=Books.csv')

import pandas as pd

books = pd.read_csv('/content/book-recommendation-dataset/Books.csv')
ratings = pd.read_csv('/content/book-recommendation-dataset/Ratings.csv')
users = pd.read_csv('/content/book-recommendation-dataset/Users.csv')

"""## Data Understading"""

def list_desc_dataframe(df):
  ListofDecs = []

  for col in df.columns:
    ListofDecs.append([col, df[col].dtype, df[col].isna().sum(), df[col].nunique(), df[col].unique()[:5]])

  df_decs = pd.DataFrame(ListofDecs, columns=['column', 'type', 'null', 'number of unique value', 'sample unique value'])
  return df_decs

"""### Dataset Books"""

books.head()

books_decs = list_desc_dataframe(books)
books_decs

print(f'Jumlah banyak data: {books.shape[0]}')

"""Berdasarkan output diatas dataset `Books.csv` memiliki data sebanyak **271360 baris** dengan terdiri dari **8 kolom**

Berikut penjelasan singkat untuk setiap kolom:

1. **ISBN**: Nomor identifikasi unik untuk setiap buku.
2. **Book-Title**: Judul buku.
3. **Book-Author**: Nama penulis buku.
4. **Year-Of-Publication**: Tahun terbit buku.
5. **Publisher**: Nama penerbit buku.
6. **Image-URL-S**: URL gambar sampul buku resolusi kecil.
7. **Image-URL-M**: URL gambar sampul buku resolusi sedang.
8. **Image-URL-L**: URL gambar sampul buku resolusi besar.

Terdapat 2-3 missing value pada kolom `Book-Author`, `Publisher`, & `Image-URL-L`.

### Dataset Ratings
"""

ratings.head()

ratings_decs = list_desc_dataframe(ratings)
ratings_decs

print(f'Jumlah banyak data: {ratings.shape[0]}')

"""Berdasarkan output diatas dataset `Ratings.csv` memiliki data sebanyak **1149780 baris** dengan terdiri dari **3 kolom**

Berikut penjelasan singkat untuk setiap kolom:

1. **User-ID**: Kode unik dari setiap users.
2. **ISBN**: Nomor identifikasi unik untuk setiap buku.
3. **Book-Rating**: Rating dari setiap buku yang diberikan oleh user.

Pada dataset ini tidak terdapat null values
"""

df_percentage = ratings.groupby('Book-Rating').size().reset_index(name='Count')
df_percentage['Percentage'] = round((df_percentage['Count']/df_percentage['Count'].sum())*100,2)
df_percentage

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(20,10))
sns.barplot(data=df_percentage, x='Book-Rating', y='Percentage')
plt.title('Persentase Rating Buku', fontsize=28, weight='bold')

"""### Dataset Users"""

users.head()

users_decs = list_desc_dataframe(users)
users_decs

print(f'Jumlah banyak data: {users.shape[0]}')

"""Berdasarkan output diatas dataset `Users.csv` memiliki data sebanyak **278858 baris** dengan terdiri dari **3 kolom**

Berikut penjelasan singkat untuk setiap kolom:

1. **User-ID**: Kode unik dari setiap users.
2. **Location**: Lokasi dari setiap users berada/tinggal
3. **Age**: Umur dari setiap user.

Terdapat 110762 missing values pada kolom `Age`.

## Data Preparation

### Merge Dataframe
"""

# Menggabungkan dataframe seluruh data frame
books_ratings = pd.merge(books, ratings, on='ISBN', how='left')
all = pd.merge(books_ratings, users, on='User-ID', how='left')
all.head()

"""Kolom yang akan kita gunakan pada case kali ini hanya `ISBN`, `Book-Title`, `Book-Author`, `Publisher`, `User-ID`, & `Book-Rating`"""

all = all[['ISBN', 'Book-Title', 'Book-Author', 'Publisher', 'User-ID', 'Book-Rating']]
all.head()

all.info()

"""### Drop Missing Values"""

all.isna().sum()

"""Terdapat nilai null pada kolom `Book-Author` dan `Publisher` sebanyak 2 data. Karena jumlahnya sangat sedikit dibandingkan total 1.032.345 baris, maka sebaiknya baris tersebut dihapus. Selain itu, kolom `User-ID` dan `Book-Rating` juga memiliki nilai null. Mengingat sistem rekomendasi sangat bergantung pada informasi `User-ID` (siapa yang memberi rating) dan `Book-Rating` (nilai yang diberikan), maka data yang tidak memiliki nilai pada kolom tersebut tidak dapat digunakan dan juga harus dihapus."""

# Menghapus null values
all.dropna(inplace=True)

all.isna().sum()

"""### Drop Invalid Values"""

all['Book-Rating'].value_counts()

"""Pada kolom `Book-Rating`, terdapat nilai 0 yang jumlahnya sangat besar. Nilai tersebut harus dihapus karena beberapa alasan:  
- Merupakan **bukan preferensi nyata** dari pengguna (biasanya menunjukkan bahwa pengguna tidak memberikan rating).  
- **Tidak berguna** dalam sistem rekomendasi berbasis rating, baik **collaborative filtering** maupun **content-based filtering**.  
- Jumlahnya yang sangat dominan dapat **menyebabkan bias** dalam pelatihan model.

"""

all = all[all['Book-Rating'] != 0]

all.sort_values('User-ID', ascending=True)

all.groupby('Book-Title').agg({
    'Book-Title' : 'count',
    'Book-Rating' : 'mean'
}).rename(columns={'Book-Title': 'Number Reads'}).sort_values(by=['Book-Rating', 'Number Reads'], ascending=[True, False])

# Count most frequent genres
top_book = all['Book-Title'].value_counts().head(10)

plt.figure(figsize=(10,6))
sns.barplot(x=top_book.values, y=top_book.index)
plt.title('Top 10 Most Read Books')
plt.xlabel('Number of Reads')
plt.ylabel('Book Title')
plt.show()

all.isna().sum()

"""### Encoding Data"""

# Mengubah kolom User-ID menjadi list tanpa nilai yang sama
user_id = all['User-ID'].unique().tolist()
print(f'list User-ID: {user_id}')

# Melakukan encoding User-ID
id_to_id_encoded = {x: i for i, x in enumerate(user_id)}
print(f'encoded User-ID: {id_to_id_encoded}')

# Melakukan proses encoding angka ke User-ID
id_encoded_to_id = {i: x for i, x in enumerate(user_id)}
print(f'encoded angka ke User-ID: {id_encoded_to_id}')

# Mengubah kolom ISBN menjadi list tanpa nilai yang sama
isbn = all['ISBN'].unique().tolist()
print(f'list ISBN: {isbn}')

# Melakukan encoding ISBN
isbn_to_isbn_encoded = {x: i for i, x in enumerate(isbn)}
print(f'encoded ISBN: {isbn_to_isbn_encoded}')

# Melakukan proses encoding angka ke ISBN
isbn_encoded_to_isbn = {i: x for i, x in enumerate(isbn)}
print(f'encoded angka ke ISBN: {isbn_encoded_to_isbn}')

# Mapping value encoded
all['user'] = all['User-ID'].map(id_to_id_encoded)
all['isbn'] = all['ISBN'].map(isbn_to_isbn_encoded)

all.sample(5)

# Mendapatkan jumlah user
num_users = len(id_to_id_encoded)
print(num_users)

# Mendapatkan jumlah book
num_isbn = len(isbn_to_isbn_encoded)
print(num_isbn)

# Mengubah rating menjadi nilai float
all['Book-Rating'] = all['Book-Rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(all['Book-Rating'])

# Nilai maksimal rating
max_rating = max(all['Book-Rating'])

print('Number of User: {}, Number of ISBN: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_isbn, min_rating, max_rating
))

"""### Randomising the Dataset

Dilakukannya pengacakan terhadap dataset yang akan digunakan agar distribusinya menjadi random.
"""

# Mengacak dataset
df = all.sample(frac=1, random_state=42)
df

"""### Spliting Data Train & Test"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
X = df[['user', 'isbn']].values
y = MinMaxScaler().fit_transform(df[['Book-Rating']])

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.2, random_state=42)

print(f'Banyaknya data pada X_train: {len(X_train)}')
print(f'Banyaknya data pada y_train: {len(y_train)}')
print(f'Banyaknya data pada X_val: {len(X_val)}')
print(f'Banyaknya data pada y_val: {len(y_val)}')

"""## Modeling"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_isbn, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_isbn = num_isbn
    self.embedding_size = embedding_size

    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias

    self.isbn_embedding = layers.Embedding( # layer embeddings isbn
        num_isbn,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.isbn_bias = layers.Embedding(num_isbn, 1) # layer embedding isbn bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    isbn_vector = self.isbn_embedding(inputs[:, 1]) # memanggil layer embedding 3
    isbn_bias = self.isbn_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_isbn = tf.tensordot(user_vector, isbn_vector, 2)

    x = dot_user_isbn + user_bias + isbn_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_isbn, 1) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""## Evaluation"""

# Memulai training

history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = 128,
    epochs = 20,
    validation_data = (X_val, y_val)
)

plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(history.history['root_mean_squared_error'], label='Training RMSE')
plt.plot(history.history['val_root_mean_squared_error'], label='Validation RMSE')
plt.title('Training and Validation RMSE')
plt.xlabel('Epochs')
plt.ylabel('RMSE')
plt.legend()
plt.show()

books_df = books
df = pd.read_csv('/content/book-recommendation-dataset/Ratings.csv')

# Mengambil sample user
user_id = df['User-ID'].sample(1).iloc[0]
book_read_by_user = df[df['User-ID'] == user_id]
book_not_read = books_df[~books_df['ISBN'].isin(book_read_by_user.ISBN.values)]['ISBN']
book_not_read = list(
    set(book_not_read)
    .intersection(set(isbn_to_isbn_encoded.keys()))
)

book_not_read = [[isbn_to_isbn_encoded.get(x)] for x in book_not_read]
user_encoder = id_to_id_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_read), book_not_read)
)

books_df.head()

ratings = model.predict(user_book_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_isbns = [
    isbn_encoded_to_isbn.get(book_not_read[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Books with high ratings from user')
print('----' * 8)

top_book_user = (
    book_read_by_user.sort_values(
        by = 'Book-Rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)

book_df_rows = books_df[books_df['ISBN'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row._3, "-", row._2)

print('----' * 8)
print('Top 10 book recommendation')
print('----' * 8)

recommended_books = books_df[books_df['ISBN'].isin(recommended_book_isbns)]
for row in recommended_books.itertuples():
    print(f"{row._3} - '{row._2}' ({row._4}). {row.Publisher}.")